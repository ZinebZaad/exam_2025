{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CACBFsndOCo"
      },
      "source": [
        "# Exercices"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Préliminaires**: Clone de votre repo et imports"
      ],
      "metadata": {
        "id": "hfkMtaHleKAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/ZinebZaad/exam_2025.git\n",
        "! cp exam_2025/utils/utils_exercices.py .\n",
        "\n",
        "import copy\n",
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "xiD_cI-geJjI",
        "outputId": "3d96bb2b-32db-41be-d652-c450a99545c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'exam_2025'...\n",
            "remote: Enumerating objects: 71, done.\u001b[K\n",
            "remote: Counting objects: 100% (71/71), done.\u001b[K\n",
            "remote: Compressing objects: 100% (63/63), done.\u001b[K\n",
            "remote: Total 71 (delta 27), reused 19 (delta 5), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (71/71), 1.41 MiB | 21.61 MiB/s, done.\n",
            "Resolving deltas: 100% (27/27), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clef personnelle pour la partie théorique**\n",
        "\n",
        "Dans la cellule suivante, choisir un entier entre 100 et 1000 (il doit être personnel). Cet entier servira de graine au générateur de nombres aléatoire a conserver pour tous les exercices.\n",
        "\n"
      ],
      "metadata": {
        "id": "J3ga_6BNc5DR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mySeed = 200"
      ],
      "metadata": {
        "id": "PrCTHM4od5UZ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "---\n",
        "\n",
        "\\"
      ],
      "metadata": {
        "id": "TRWBLVpCWC06"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5RcggmAkJLV"
      },
      "source": [
        "\\\n",
        "\n",
        "**Exercice 1** *Une relation linéaire*\n",
        "\n",
        "La fonction *generate_dataset* fournit deux jeux de données (entraînement et test). Pour chaque jeu de données, la clef 'inputs' donne accès à un tableau numpy (numpy array) de prédicteurs empilés horizontalement : chaque ligne $i$ contient trois prédicteurs $x_i$, $y_i$ et $z_i$. La clef 'targets' renvoie le vecteur des cibles $t_i$. \\\n",
        "\n",
        "Les cibles sont liées aux prédicteurs par le modèle:\n",
        "$$ t = \\theta_0 + \\theta_1 x + \\theta_2 y + \\theta_3 z + \\epsilon$$ où $\\epsilon \\sim \\mathcal{N}(0,\\eta)$\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from utils_exercices import generate_dataset, Dataset1\n",
        "train_set, test_set = generate_dataset(mySeed)"
      ],
      "metadata": {
        "id": "gEQmgTI8my8i"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** Par quelle méthode simple peut-on estimer les coefficients $\\theta_k$ ? La mettre en oeuvre avec la librairie python de votre choix."
      ],
      "metadata": {
        "id": "q5XZTrXNk12K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANSWER :\n",
        "We can estimate  $\\theta_k$ coefficients with Linear Regression. This approach minimizes the sum of squared differences between the observed and predicted target values."
      ],
      "metadata": {
        "id": "N5Xzt49YXEZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "# Extract inputs and targets from the training set\n",
        "X_train = train_set['inputs']\n",
        "y_train = train_set['targets']\n",
        "\n",
        "# Train a linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Retrieve the coefficients\n",
        "theta0 = model.intercept_  # This is θ0\n",
        "theta1, theta2, theta3 = model.coef_  # These are θ1, θ2, θ3\n",
        "\n",
        "# Display the results\n",
        "print(f\"Estimated coefficients:\")\n",
        "print(f\"θ0 (Intercept): {theta0}\")\n",
        "print(f\"θ1 (x): {theta1}\")\n",
        "print(f\"θ2 (y): {theta2}\")\n",
        "print(f\"θ3 (z): {theta3}\")\n"
      ],
      "metadata": {
        "id": "HITtUqHhFMkn",
        "outputId": "eed40b21-e41c-46bd-94ef-5914339cbde6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated coefficients:\n",
            "θ0 (Intercept): 10.078764034363882\n",
            "θ1 (x): 1.9515686197527802\n",
            "θ2 (y): 1.9484222058962641\n",
            "θ3 (z): 3.599666992319566\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MXGXg8tlPULY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** Dans les cellules suivantes, on se propose d'estimer les $\\theta_k$ grâce à un réseau de neurones entraîné par SGD. Quelle architecture s'y prête ? Justifier en termes d'expressivité et de performances en généralisation puis la coder dans la cellule suivante."
      ],
      "metadata": {
        "id": "CH_Z5ZEIlQPE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANSWER :\n",
        "The suitable architecture is a single-layer feedforward neural network with one fully connected layer, taking 3 inputs (\\(x, y, z\\)) and producing 1 output (\\(t\\)). This structure directly models the linear relationship \\(t = \\theta_0 + \\theta_1x + \\theta_2y + \\theta_3z\\) through a simple linear transformation. It is expressive enough for the task, avoids overfitting, and genera\n"
      ],
      "metadata": {
        "id": "broUNeK0Y9fQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset et dataloader :\n",
        "dataset = Dataset1(train_set['inputs'], train_set['targets'])\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=100, shuffle=True)\n",
        "\n",
        "# A coder :\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.fc = nn.Linear(3, 1)  # Linear layer for 3 inputs (x, y, z) and 1 output (t)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)  # Return the linear combination"
      ],
      "metadata": {
        "id": "PPx543blnxdb"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Entraîner cette architecture à la tâche de régression définie par les entrées et sorties du jeu d'entraînement (compléter la cellule ci-dessous)."
      ],
      "metadata": {
        "id": "g6BSTBitpGBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, loss, and optimizer\n",
        "mySimpleNet = SimpleNet()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(mySimpleNet.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 500\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0.0  # Track total loss for this epoch\n",
        "    for batch_inputs, batch_targets in dataloader:\n",
        "        optimizer.zero_grad()  # Reset gradients to zero\n",
        "\n",
        "        # Forward pass: compute predictions\n",
        "        outputs = mySimpleNet(batch_inputs)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion(outputs.squeeze(), batch_targets)\n",
        "\n",
        "        # Backward pass: compute gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate loss for reporting\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Print the epoch loss\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}\")"
      ],
      "metadata": {
        "id": "Wjfa2Z4RoPO-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b396c984-70b2-4229-e5e1-f747cf123dbf"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/500], Loss: 1151.4295\n",
            "Epoch [2/500], Loss: 695.2972\n",
            "Epoch [3/500], Loss: 428.3824\n",
            "Epoch [4/500], Loss: 271.8053\n",
            "Epoch [5/500], Loss: 179.7733\n",
            "Epoch [6/500], Loss: 125.3682\n",
            "Epoch [7/500], Loss: 93.0775\n",
            "Epoch [8/500], Loss: 73.7825\n",
            "Epoch [9/500], Loss: 62.1863\n",
            "Epoch [10/500], Loss: 55.0772\n",
            "Epoch [11/500], Loss: 50.6823\n",
            "Epoch [12/500], Loss: 47.8589\n",
            "Epoch [13/500], Loss: 46.0159\n",
            "Epoch [14/500], Loss: 44.7323\n",
            "Epoch [15/500], Loss: 43.8329\n",
            "Epoch [16/500], Loss: 43.1638\n",
            "Epoch [17/500], Loss: 42.6275\n",
            "Epoch [18/500], Loss: 42.1914\n",
            "Epoch [19/500], Loss: 41.8368\n",
            "Epoch [20/500], Loss: 41.5214\n",
            "Epoch [21/500], Loss: 41.2431\n",
            "Epoch [22/500], Loss: 40.9951\n",
            "Epoch [23/500], Loss: 40.7761\n",
            "Epoch [24/500], Loss: 40.5731\n",
            "Epoch [25/500], Loss: 40.3836\n",
            "Epoch [26/500], Loss: 40.2241\n",
            "Epoch [27/500], Loss: 40.0671\n",
            "Epoch [28/500], Loss: 39.9297\n",
            "Epoch [29/500], Loss: 39.7999\n",
            "Epoch [30/500], Loss: 39.6879\n",
            "Epoch [31/500], Loss: 39.5716\n",
            "Epoch [32/500], Loss: 39.4821\n",
            "Epoch [33/500], Loss: 39.3823\n",
            "Epoch [34/500], Loss: 39.3014\n",
            "Epoch [35/500], Loss: 39.2391\n",
            "Epoch [36/500], Loss: 39.1599\n",
            "Epoch [37/500], Loss: 39.1034\n",
            "Epoch [38/500], Loss: 39.0345\n",
            "Epoch [39/500], Loss: 38.9862\n",
            "Epoch [40/500], Loss: 38.9306\n",
            "Epoch [41/500], Loss: 38.8845\n",
            "Epoch [42/500], Loss: 38.8539\n",
            "Epoch [43/500], Loss: 38.8027\n",
            "Epoch [44/500], Loss: 38.7739\n",
            "Epoch [45/500], Loss: 38.7422\n",
            "Epoch [46/500], Loss: 38.7063\n",
            "Epoch [47/500], Loss: 38.6808\n",
            "Epoch [48/500], Loss: 38.6574\n",
            "Epoch [49/500], Loss: 38.6331\n",
            "Epoch [50/500], Loss: 38.6170\n",
            "Epoch [51/500], Loss: 38.5927\n",
            "Epoch [52/500], Loss: 38.5755\n",
            "Epoch [53/500], Loss: 38.5671\n",
            "Epoch [54/500], Loss: 38.5444\n",
            "Epoch [55/500], Loss: 38.5272\n",
            "Epoch [56/500], Loss: 38.5223\n",
            "Epoch [57/500], Loss: 38.5126\n",
            "Epoch [58/500], Loss: 38.5051\n",
            "Epoch [59/500], Loss: 38.4803\n",
            "Epoch [60/500], Loss: 38.4729\n",
            "Epoch [61/500], Loss: 38.4762\n",
            "Epoch [62/500], Loss: 38.4662\n",
            "Epoch [63/500], Loss: 38.4572\n",
            "Epoch [64/500], Loss: 38.4469\n",
            "Epoch [65/500], Loss: 38.4391\n",
            "Epoch [66/500], Loss: 38.4389\n",
            "Epoch [67/500], Loss: 38.4394\n",
            "Epoch [68/500], Loss: 38.4354\n",
            "Epoch [69/500], Loss: 38.4322\n",
            "Epoch [70/500], Loss: 38.4204\n",
            "Epoch [71/500], Loss: 38.4202\n",
            "Epoch [72/500], Loss: 38.4112\n",
            "Epoch [73/500], Loss: 38.4043\n",
            "Epoch [74/500], Loss: 38.4066\n",
            "Epoch [75/500], Loss: 38.4043\n",
            "Epoch [76/500], Loss: 38.4003\n",
            "Epoch [77/500], Loss: 38.4058\n",
            "Epoch [78/500], Loss: 38.4098\n",
            "Epoch [79/500], Loss: 38.3977\n",
            "Epoch [80/500], Loss: 38.3886\n",
            "Epoch [81/500], Loss: 38.3849\n",
            "Epoch [82/500], Loss: 38.3917\n",
            "Epoch [83/500], Loss: 38.3923\n",
            "Epoch [84/500], Loss: 38.3945\n",
            "Epoch [85/500], Loss: 38.3928\n",
            "Epoch [86/500], Loss: 38.3929\n",
            "Epoch [87/500], Loss: 38.3819\n",
            "Epoch [88/500], Loss: 38.3923\n",
            "Epoch [89/500], Loss: 38.3816\n",
            "Epoch [90/500], Loss: 38.3860\n",
            "Epoch [91/500], Loss: 38.3790\n",
            "Epoch [92/500], Loss: 38.3839\n",
            "Epoch [93/500], Loss: 38.3832\n",
            "Epoch [94/500], Loss: 38.3921\n",
            "Epoch [95/500], Loss: 38.3950\n",
            "Epoch [96/500], Loss: 38.3802\n",
            "Epoch [97/500], Loss: 38.3791\n",
            "Epoch [98/500], Loss: 38.3785\n",
            "Epoch [99/500], Loss: 38.3850\n",
            "Epoch [100/500], Loss: 38.3787\n",
            "Epoch [101/500], Loss: 38.3782\n",
            "Epoch [102/500], Loss: 38.3858\n",
            "Epoch [103/500], Loss: 38.3785\n",
            "Epoch [104/500], Loss: 38.3738\n",
            "Epoch [105/500], Loss: 38.3820\n",
            "Epoch [106/500], Loss: 38.3871\n",
            "Epoch [107/500], Loss: 38.3869\n",
            "Epoch [108/500], Loss: 38.3848\n",
            "Epoch [109/500], Loss: 38.3695\n",
            "Epoch [110/500], Loss: 38.3730\n",
            "Epoch [111/500], Loss: 38.3740\n",
            "Epoch [112/500], Loss: 38.3869\n",
            "Epoch [113/500], Loss: 38.3678\n",
            "Epoch [114/500], Loss: 38.3839\n",
            "Epoch [115/500], Loss: 38.3839\n",
            "Epoch [116/500], Loss: 38.3774\n",
            "Epoch [117/500], Loss: 38.3747\n",
            "Epoch [118/500], Loss: 38.3769\n",
            "Epoch [119/500], Loss: 38.3842\n",
            "Epoch [120/500], Loss: 38.3889\n",
            "Epoch [121/500], Loss: 38.3669\n",
            "Epoch [122/500], Loss: 38.3705\n",
            "Epoch [123/500], Loss: 38.3745\n",
            "Epoch [124/500], Loss: 38.3800\n",
            "Epoch [125/500], Loss: 38.3838\n",
            "Epoch [126/500], Loss: 38.3733\n",
            "Epoch [127/500], Loss: 38.3712\n",
            "Epoch [128/500], Loss: 38.3760\n",
            "Epoch [129/500], Loss: 38.3761\n",
            "Epoch [130/500], Loss: 38.3814\n",
            "Epoch [131/500], Loss: 38.3775\n",
            "Epoch [132/500], Loss: 38.3804\n",
            "Epoch [133/500], Loss: 38.3733\n",
            "Epoch [134/500], Loss: 38.3719\n",
            "Epoch [135/500], Loss: 38.3894\n",
            "Epoch [136/500], Loss: 38.3875\n",
            "Epoch [137/500], Loss: 38.3772\n",
            "Epoch [138/500], Loss: 38.3710\n",
            "Epoch [139/500], Loss: 38.3826\n",
            "Epoch [140/500], Loss: 38.3827\n",
            "Epoch [141/500], Loss: 38.3886\n",
            "Epoch [142/500], Loss: 38.3808\n",
            "Epoch [143/500], Loss: 38.3706\n",
            "Epoch [144/500], Loss: 38.3923\n",
            "Epoch [145/500], Loss: 38.3761\n",
            "Epoch [146/500], Loss: 38.3715\n",
            "Epoch [147/500], Loss: 38.3814\n",
            "Epoch [148/500], Loss: 38.3786\n",
            "Epoch [149/500], Loss: 38.3731\n",
            "Epoch [150/500], Loss: 38.3689\n",
            "Epoch [151/500], Loss: 38.3761\n",
            "Epoch [152/500], Loss: 38.3804\n",
            "Epoch [153/500], Loss: 38.3822\n",
            "Epoch [154/500], Loss: 38.3786\n",
            "Epoch [155/500], Loss: 38.3728\n",
            "Epoch [156/500], Loss: 38.3736\n",
            "Epoch [157/500], Loss: 38.3897\n",
            "Epoch [158/500], Loss: 38.3729\n",
            "Epoch [159/500], Loss: 38.3778\n",
            "Epoch [160/500], Loss: 38.3761\n",
            "Epoch [161/500], Loss: 38.3878\n",
            "Epoch [162/500], Loss: 38.3819\n",
            "Epoch [163/500], Loss: 38.3818\n",
            "Epoch [164/500], Loss: 38.3887\n",
            "Epoch [165/500], Loss: 38.3809\n",
            "Epoch [166/500], Loss: 38.3751\n",
            "Epoch [167/500], Loss: 38.3720\n",
            "Epoch [168/500], Loss: 38.3757\n",
            "Epoch [169/500], Loss: 38.3843\n",
            "Epoch [170/500], Loss: 38.3858\n",
            "Epoch [171/500], Loss: 38.3869\n",
            "Epoch [172/500], Loss: 38.3741\n",
            "Epoch [173/500], Loss: 38.3788\n",
            "Epoch [174/500], Loss: 38.3894\n",
            "Epoch [175/500], Loss: 38.3698\n",
            "Epoch [176/500], Loss: 38.3796\n",
            "Epoch [177/500], Loss: 38.3711\n",
            "Epoch [178/500], Loss: 38.3900\n",
            "Epoch [179/500], Loss: 38.3651\n",
            "Epoch [180/500], Loss: 38.3788\n",
            "Epoch [181/500], Loss: 38.3741\n",
            "Epoch [182/500], Loss: 38.3771\n",
            "Epoch [183/500], Loss: 38.3867\n",
            "Epoch [184/500], Loss: 38.3684\n",
            "Epoch [185/500], Loss: 38.3776\n",
            "Epoch [186/500], Loss: 38.3814\n",
            "Epoch [187/500], Loss: 38.3716\n",
            "Epoch [188/500], Loss: 38.3710\n",
            "Epoch [189/500], Loss: 38.3701\n",
            "Epoch [190/500], Loss: 38.3702\n",
            "Epoch [191/500], Loss: 38.3782\n",
            "Epoch [192/500], Loss: 38.3805\n",
            "Epoch [193/500], Loss: 38.3859\n",
            "Epoch [194/500], Loss: 38.3753\n",
            "Epoch [195/500], Loss: 38.3714\n",
            "Epoch [196/500], Loss: 38.3886\n",
            "Epoch [197/500], Loss: 38.3699\n",
            "Epoch [198/500], Loss: 38.3739\n",
            "Epoch [199/500], Loss: 38.3689\n",
            "Epoch [200/500], Loss: 38.3815\n",
            "Epoch [201/500], Loss: 38.3803\n",
            "Epoch [202/500], Loss: 38.3733\n",
            "Epoch [203/500], Loss: 38.3696\n",
            "Epoch [204/500], Loss: 38.3724\n",
            "Epoch [205/500], Loss: 38.3704\n",
            "Epoch [206/500], Loss: 38.3754\n",
            "Epoch [207/500], Loss: 38.3719\n",
            "Epoch [208/500], Loss: 38.3736\n",
            "Epoch [209/500], Loss: 38.3812\n",
            "Epoch [210/500], Loss: 38.3915\n",
            "Epoch [211/500], Loss: 38.3740\n",
            "Epoch [212/500], Loss: 38.3785\n",
            "Epoch [213/500], Loss: 38.3737\n",
            "Epoch [214/500], Loss: 38.3827\n",
            "Epoch [215/500], Loss: 38.3736\n",
            "Epoch [216/500], Loss: 38.3795\n",
            "Epoch [217/500], Loss: 38.3729\n",
            "Epoch [218/500], Loss: 38.3753\n",
            "Epoch [219/500], Loss: 38.3838\n",
            "Epoch [220/500], Loss: 38.3804\n",
            "Epoch [221/500], Loss: 38.3683\n",
            "Epoch [222/500], Loss: 38.3824\n",
            "Epoch [223/500], Loss: 38.3754\n",
            "Epoch [224/500], Loss: 38.3756\n",
            "Epoch [225/500], Loss: 38.3763\n",
            "Epoch [226/500], Loss: 38.3752\n",
            "Epoch [227/500], Loss: 38.3739\n",
            "Epoch [228/500], Loss: 38.3770\n",
            "Epoch [229/500], Loss: 38.3828\n",
            "Epoch [230/500], Loss: 38.3812\n",
            "Epoch [231/500], Loss: 38.3753\n",
            "Epoch [232/500], Loss: 38.3819\n",
            "Epoch [233/500], Loss: 38.3726\n",
            "Epoch [234/500], Loss: 38.3908\n",
            "Epoch [235/500], Loss: 38.3784\n",
            "Epoch [236/500], Loss: 38.3611\n",
            "Epoch [237/500], Loss: 38.3777\n",
            "Epoch [238/500], Loss: 38.3858\n",
            "Epoch [239/500], Loss: 38.3779\n",
            "Epoch [240/500], Loss: 38.3782\n",
            "Epoch [241/500], Loss: 38.3832\n",
            "Epoch [242/500], Loss: 38.3835\n",
            "Epoch [243/500], Loss: 38.3771\n",
            "Epoch [244/500], Loss: 38.3760\n",
            "Epoch [245/500], Loss: 38.3833\n",
            "Epoch [246/500], Loss: 38.3784\n",
            "Epoch [247/500], Loss: 38.3728\n",
            "Epoch [248/500], Loss: 38.3778\n",
            "Epoch [249/500], Loss: 38.3821\n",
            "Epoch [250/500], Loss: 38.3780\n",
            "Epoch [251/500], Loss: 38.3768\n",
            "Epoch [252/500], Loss: 38.3737\n",
            "Epoch [253/500], Loss: 38.3835\n",
            "Epoch [254/500], Loss: 38.3857\n",
            "Epoch [255/500], Loss: 38.3682\n",
            "Epoch [256/500], Loss: 38.3705\n",
            "Epoch [257/500], Loss: 38.3746\n",
            "Epoch [258/500], Loss: 38.3858\n",
            "Epoch [259/500], Loss: 38.3718\n",
            "Epoch [260/500], Loss: 38.3783\n",
            "Epoch [261/500], Loss: 38.3769\n",
            "Epoch [262/500], Loss: 38.3782\n",
            "Epoch [263/500], Loss: 38.3702\n",
            "Epoch [264/500], Loss: 38.3821\n",
            "Epoch [265/500], Loss: 38.3703\n",
            "Epoch [266/500], Loss: 38.3761\n",
            "Epoch [267/500], Loss: 38.3733\n",
            "Epoch [268/500], Loss: 38.3872\n",
            "Epoch [269/500], Loss: 38.3785\n",
            "Epoch [270/500], Loss: 38.3688\n",
            "Epoch [271/500], Loss: 38.3936\n",
            "Epoch [272/500], Loss: 38.3766\n",
            "Epoch [273/500], Loss: 38.3794\n",
            "Epoch [274/500], Loss: 38.3848\n",
            "Epoch [275/500], Loss: 38.3748\n",
            "Epoch [276/500], Loss: 38.3811\n",
            "Epoch [277/500], Loss: 38.3768\n",
            "Epoch [278/500], Loss: 38.3831\n",
            "Epoch [279/500], Loss: 38.3744\n",
            "Epoch [280/500], Loss: 38.3749\n",
            "Epoch [281/500], Loss: 38.3817\n",
            "Epoch [282/500], Loss: 38.3774\n",
            "Epoch [283/500], Loss: 38.3795\n",
            "Epoch [284/500], Loss: 38.3755\n",
            "Epoch [285/500], Loss: 38.3789\n",
            "Epoch [286/500], Loss: 38.3772\n",
            "Epoch [287/500], Loss: 38.3779\n",
            "Epoch [288/500], Loss: 38.3913\n",
            "Epoch [289/500], Loss: 38.3801\n",
            "Epoch [290/500], Loss: 38.3756\n",
            "Epoch [291/500], Loss: 38.3861\n",
            "Epoch [292/500], Loss: 38.3760\n",
            "Epoch [293/500], Loss: 38.3737\n",
            "Epoch [294/500], Loss: 38.3783\n",
            "Epoch [295/500], Loss: 38.3784\n",
            "Epoch [296/500], Loss: 38.3812\n",
            "Epoch [297/500], Loss: 38.3795\n",
            "Epoch [298/500], Loss: 38.3767\n",
            "Epoch [299/500], Loss: 38.3788\n",
            "Epoch [300/500], Loss: 38.3752\n",
            "Epoch [301/500], Loss: 38.3741\n",
            "Epoch [302/500], Loss: 38.3767\n",
            "Epoch [303/500], Loss: 38.3790\n",
            "Epoch [304/500], Loss: 38.3835\n",
            "Epoch [305/500], Loss: 38.3748\n",
            "Epoch [306/500], Loss: 38.3712\n",
            "Epoch [307/500], Loss: 38.3740\n",
            "Epoch [308/500], Loss: 38.3673\n",
            "Epoch [309/500], Loss: 38.3867\n",
            "Epoch [310/500], Loss: 38.3780\n",
            "Epoch [311/500], Loss: 38.3752\n",
            "Epoch [312/500], Loss: 38.3769\n",
            "Epoch [313/500], Loss: 38.3741\n",
            "Epoch [314/500], Loss: 38.3696\n",
            "Epoch [315/500], Loss: 38.3801\n",
            "Epoch [316/500], Loss: 38.3845\n",
            "Epoch [317/500], Loss: 38.3729\n",
            "Epoch [318/500], Loss: 38.3891\n",
            "Epoch [319/500], Loss: 38.3750\n",
            "Epoch [320/500], Loss: 38.3829\n",
            "Epoch [321/500], Loss: 38.3801\n",
            "Epoch [322/500], Loss: 38.3788\n",
            "Epoch [323/500], Loss: 38.3823\n",
            "Epoch [324/500], Loss: 38.3723\n",
            "Epoch [325/500], Loss: 38.3805\n",
            "Epoch [326/500], Loss: 38.3820\n",
            "Epoch [327/500], Loss: 38.3769\n",
            "Epoch [328/500], Loss: 38.3680\n",
            "Epoch [329/500], Loss: 38.3841\n",
            "Epoch [330/500], Loss: 38.3798\n",
            "Epoch [331/500], Loss: 38.3821\n",
            "Epoch [332/500], Loss: 38.3827\n",
            "Epoch [333/500], Loss: 38.3746\n",
            "Epoch [334/500], Loss: 38.3687\n",
            "Epoch [335/500], Loss: 38.3824\n",
            "Epoch [336/500], Loss: 38.3717\n",
            "Epoch [337/500], Loss: 38.3813\n",
            "Epoch [338/500], Loss: 38.3889\n",
            "Epoch [339/500], Loss: 38.3708\n",
            "Epoch [340/500], Loss: 38.3768\n",
            "Epoch [341/500], Loss: 38.3822\n",
            "Epoch [342/500], Loss: 38.3748\n",
            "Epoch [343/500], Loss: 38.3770\n",
            "Epoch [344/500], Loss: 38.3925\n",
            "Epoch [345/500], Loss: 38.3732\n",
            "Epoch [346/500], Loss: 38.3712\n",
            "Epoch [347/500], Loss: 38.3746\n",
            "Epoch [348/500], Loss: 38.3742\n",
            "Epoch [349/500], Loss: 38.3893\n",
            "Epoch [350/500], Loss: 38.3741\n",
            "Epoch [351/500], Loss: 38.3763\n",
            "Epoch [352/500], Loss: 38.3764\n",
            "Epoch [353/500], Loss: 38.3775\n",
            "Epoch [354/500], Loss: 38.3738\n",
            "Epoch [355/500], Loss: 38.3789\n",
            "Epoch [356/500], Loss: 38.3782\n",
            "Epoch [357/500], Loss: 38.3759\n",
            "Epoch [358/500], Loss: 38.3900\n",
            "Epoch [359/500], Loss: 38.3846\n",
            "Epoch [360/500], Loss: 38.3865\n",
            "Epoch [361/500], Loss: 38.3738\n",
            "Epoch [362/500], Loss: 38.3807\n",
            "Epoch [363/500], Loss: 38.3759\n",
            "Epoch [364/500], Loss: 38.3747\n",
            "Epoch [365/500], Loss: 38.3778\n",
            "Epoch [366/500], Loss: 38.3757\n",
            "Epoch [367/500], Loss: 38.3793\n",
            "Epoch [368/500], Loss: 38.3817\n",
            "Epoch [369/500], Loss: 38.3671\n",
            "Epoch [370/500], Loss: 38.3785\n",
            "Epoch [371/500], Loss: 38.3872\n",
            "Epoch [372/500], Loss: 38.3770\n",
            "Epoch [373/500], Loss: 38.3774\n",
            "Epoch [374/500], Loss: 38.3878\n",
            "Epoch [375/500], Loss: 38.3832\n",
            "Epoch [376/500], Loss: 38.3816\n",
            "Epoch [377/500], Loss: 38.3712\n",
            "Epoch [378/500], Loss: 38.3809\n",
            "Epoch [379/500], Loss: 38.3768\n",
            "Epoch [380/500], Loss: 38.3775\n",
            "Epoch [381/500], Loss: 38.3838\n",
            "Epoch [382/500], Loss: 38.3836\n",
            "Epoch [383/500], Loss: 38.3842\n",
            "Epoch [384/500], Loss: 38.3806\n",
            "Epoch [385/500], Loss: 38.3715\n",
            "Epoch [386/500], Loss: 38.3738\n",
            "Epoch [387/500], Loss: 38.3707\n",
            "Epoch [388/500], Loss: 38.3754\n",
            "Epoch [389/500], Loss: 38.3788\n",
            "Epoch [390/500], Loss: 38.3741\n",
            "Epoch [391/500], Loss: 38.3735\n",
            "Epoch [392/500], Loss: 38.3904\n",
            "Epoch [393/500], Loss: 38.3764\n",
            "Epoch [394/500], Loss: 38.3681\n",
            "Epoch [395/500], Loss: 38.3733\n",
            "Epoch [396/500], Loss: 38.3751\n",
            "Epoch [397/500], Loss: 38.3796\n",
            "Epoch [398/500], Loss: 38.3835\n",
            "Epoch [399/500], Loss: 38.3722\n",
            "Epoch [400/500], Loss: 38.3746\n",
            "Epoch [401/500], Loss: 38.3772\n",
            "Epoch [402/500], Loss: 38.3813\n",
            "Epoch [403/500], Loss: 38.3783\n",
            "Epoch [404/500], Loss: 38.3732\n",
            "Epoch [405/500], Loss: 38.3725\n",
            "Epoch [406/500], Loss: 38.3866\n",
            "Epoch [407/500], Loss: 38.3819\n",
            "Epoch [408/500], Loss: 38.3765\n",
            "Epoch [409/500], Loss: 38.3944\n",
            "Epoch [410/500], Loss: 38.3767\n",
            "Epoch [411/500], Loss: 38.3765\n",
            "Epoch [412/500], Loss: 38.3840\n",
            "Epoch [413/500], Loss: 38.3797\n",
            "Epoch [414/500], Loss: 38.3711\n",
            "Epoch [415/500], Loss: 38.3842\n",
            "Epoch [416/500], Loss: 38.3825\n",
            "Epoch [417/500], Loss: 38.3812\n",
            "Epoch [418/500], Loss: 38.3804\n",
            "Epoch [419/500], Loss: 38.3814\n",
            "Epoch [420/500], Loss: 38.3709\n",
            "Epoch [421/500], Loss: 38.3742\n",
            "Epoch [422/500], Loss: 38.3718\n",
            "Epoch [423/500], Loss: 38.3890\n",
            "Epoch [424/500], Loss: 38.3793\n",
            "Epoch [425/500], Loss: 38.3811\n",
            "Epoch [426/500], Loss: 38.3726\n",
            "Epoch [427/500], Loss: 38.3834\n",
            "Epoch [428/500], Loss: 38.3826\n",
            "Epoch [429/500], Loss: 38.3816\n",
            "Epoch [430/500], Loss: 38.3836\n",
            "Epoch [431/500], Loss: 38.3912\n",
            "Epoch [432/500], Loss: 38.3835\n",
            "Epoch [433/500], Loss: 38.3799\n",
            "Epoch [434/500], Loss: 38.3877\n",
            "Epoch [435/500], Loss: 38.3832\n",
            "Epoch [436/500], Loss: 38.3772\n",
            "Epoch [437/500], Loss: 38.3709\n",
            "Epoch [438/500], Loss: 38.3743\n",
            "Epoch [439/500], Loss: 38.3857\n",
            "Epoch [440/500], Loss: 38.3762\n",
            "Epoch [441/500], Loss: 38.3928\n",
            "Epoch [442/500], Loss: 38.3857\n",
            "Epoch [443/500], Loss: 38.3783\n",
            "Epoch [444/500], Loss: 38.3815\n",
            "Epoch [445/500], Loss: 38.3737\n",
            "Epoch [446/500], Loss: 38.3792\n",
            "Epoch [447/500], Loss: 38.3684\n",
            "Epoch [448/500], Loss: 38.3723\n",
            "Epoch [449/500], Loss: 38.3848\n",
            "Epoch [450/500], Loss: 38.3822\n",
            "Epoch [451/500], Loss: 38.3779\n",
            "Epoch [452/500], Loss: 38.3761\n",
            "Epoch [453/500], Loss: 38.3707\n",
            "Epoch [454/500], Loss: 38.3691\n",
            "Epoch [455/500], Loss: 38.3758\n",
            "Epoch [456/500], Loss: 38.3757\n",
            "Epoch [457/500], Loss: 38.3864\n",
            "Epoch [458/500], Loss: 38.3795\n",
            "Epoch [459/500], Loss: 38.3800\n",
            "Epoch [460/500], Loss: 38.3720\n",
            "Epoch [461/500], Loss: 38.3803\n",
            "Epoch [462/500], Loss: 38.3796\n",
            "Epoch [463/500], Loss: 38.3832\n",
            "Epoch [464/500], Loss: 38.3778\n",
            "Epoch [465/500], Loss: 38.3786\n",
            "Epoch [466/500], Loss: 38.3845\n",
            "Epoch [467/500], Loss: 38.3713\n",
            "Epoch [468/500], Loss: 38.3760\n",
            "Epoch [469/500], Loss: 38.3824\n",
            "Epoch [470/500], Loss: 38.3707\n",
            "Epoch [471/500], Loss: 38.3840\n",
            "Epoch [472/500], Loss: 38.3770\n",
            "Epoch [473/500], Loss: 38.3917\n",
            "Epoch [474/500], Loss: 38.3802\n",
            "Epoch [475/500], Loss: 38.3701\n",
            "Epoch [476/500], Loss: 38.3848\n",
            "Epoch [477/500], Loss: 38.3723\n",
            "Epoch [478/500], Loss: 38.3750\n",
            "Epoch [479/500], Loss: 38.3763\n",
            "Epoch [480/500], Loss: 38.3838\n",
            "Epoch [481/500], Loss: 38.3756\n",
            "Epoch [482/500], Loss: 38.3790\n",
            "Epoch [483/500], Loss: 38.3735\n",
            "Epoch [484/500], Loss: 38.3786\n",
            "Epoch [485/500], Loss: 38.3751\n",
            "Epoch [486/500], Loss: 38.3793\n",
            "Epoch [487/500], Loss: 38.3745\n",
            "Epoch [488/500], Loss: 38.3807\n",
            "Epoch [489/500], Loss: 38.3767\n",
            "Epoch [490/500], Loss: 38.3888\n",
            "Epoch [491/500], Loss: 38.3733\n",
            "Epoch [492/500], Loss: 38.3741\n",
            "Epoch [493/500], Loss: 38.3794\n",
            "Epoch [494/500], Loss: 38.3778\n",
            "Epoch [495/500], Loss: 38.3699\n",
            "Epoch [496/500], Loss: 38.3741\n",
            "Epoch [497/500], Loss: 38.3826\n",
            "Epoch [498/500], Loss: 38.3772\n",
            "Epoch [499/500], Loss: 38.3725\n",
            "Epoch [500/500], Loss: 38.3824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4** Où sont alors stockées les estimations des  $\\theta_k$ ? Les extraire du réseau *mySimpleNet* dans la cellule suivante."
      ],
      "metadata": {
        "id": "OZwKogEEp2Fr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANSWER :\n",
        "The estimated $\\theta_k$ values (model parameters) are stored in the weights and bias of the linear layer in the mySimpleNet model.\n"
      ],
      "metadata": {
        "id": "ya7AQhfIakdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the trained parameters (weights and bias) from the model\n",
        "theta0 = mySimpleNet.fc.bias.item()  # Intercept (θ0)\n",
        "theta1, theta2, theta3 = mySimpleNet.fc.weight[0].detach().numpy()  # Coefficients (θ1, θ2, θ3)\n",
        "\n",
        "# Print the estimated parameters\n",
        "print(\"Estimated coefficients:\")\n",
        "print(f\"θ0 (Intercept): {theta0}\")\n",
        "print(f\"θ1 (x): {theta1}\")\n",
        "print(f\"θ2 (y): {theta2}\")\n",
        "print(f\"θ3 (z): {theta3}\")\n"
      ],
      "metadata": {
        "id": "EjgWp1y1rseb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5467981a-5de7-4fa1-8d90-c8438993aa32"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated coefficients:\n",
            "θ0 (Intercept): 10.079853057861328\n",
            "θ1 (x): 1.951642632484436\n",
            "θ2 (y): 1.9474536180496216\n",
            "θ3 (z): 3.600342273712158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5** Tester ces estimations sur le jeu de test et comparer avec celles de la question 1. Commentez."
      ],
      "metadata": {
        "id": "pEB-V-oOrJED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract inputs and targets from the test set\n",
        "X_test = test_set['inputs']\n",
        "y_test = test_set['targets']\n",
        "\n",
        "# Linear Regression Predictions\n",
        "y_pred_lr = model.predict(X_test)\n",
        "mse_lr = ((y_pred_lr - y_test) ** 2).mean()\n",
        "\n",
        "# Neural Network Predictions\n",
        "mySimpleNet.eval()  # Set the neural network to evaluation mode\n",
        "with torch.no_grad():  # Disable gradient computation\n",
        "    y_pred_nn = mySimpleNet(torch.tensor(X_test, dtype=torch.float32)).squeeze().numpy()\n",
        "mse_nn = ((y_pred_nn - y_test) ** 2).mean()\n",
        "\n",
        "# Compare the Mean Squared Errors\n",
        "print(f\"Linear Regression Test MSE: {mse_lr:.4f}\")\n",
        "print(f\"Neural Network Test MSE: {mse_nn:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lFO3T5QbWnf",
        "outputId": "2c5b89bf-745a-4a96-aec1-f3eb35df9a3a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression Test MSE: 4.0093\n",
            "Neural Network Test MSE: 4.0089\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both models have effectively captured the underlying linear relationship defined in the dataset, with identical performance on the test set. The neural network approach demonstrates its flexibility but, for linear problems, traditional linear regression remains simpler and computationally efficient."
      ],
      "metadata": {
        "id": "kK7Y9MwpbprL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "---\n",
        "\n",
        "\\"
      ],
      "metadata": {
        "id": "VvV2jIrBNtzf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercice 2** *Champ réceptif et prédiction causale*"
      ],
      "metadata": {
        "id": "CpRvXCaAtsIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le réseau défini dans la cellule suivante est utilisé pour faire le lien entre les valeurs $(x_{t' \\leq t})$ d'une série temporelle d'entrée et la valeur présente $y_t$ d'une série temporelle cible."
      ],
      "metadata": {
        "id": "8JG9wTfK5TBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from utils_exercices import Outconv, Up_causal, Down_causal\n",
        "\n",
        "class Double_conv_causal(nn.Module):\n",
        "    '''(conv => BN => ReLU) * 2, with causal convolutions that preserve input size'''\n",
        "    def __init__(self, in_ch, out_ch, kernel_size=3, dilation=1):\n",
        "        super(Double_conv_causal, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.dilation = dilation\n",
        "        self.conv1 = nn.Conv1d(in_ch, out_ch, kernel_size=kernel_size, padding=0, dilation=dilation)\n",
        "        self.bn1 = nn.BatchNorm1d(out_ch)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv1d(out_ch, out_ch, kernel_size=kernel_size, padding=0, dilation=dilation)\n",
        "        self.bn2 = nn.BatchNorm1d(out_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.pad(x, ((self.kernel_size - 1) * self.dilation, 0))\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = F.pad(x, ((self.kernel_size - 1) * self.dilation, 0))\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class causalFCN(nn.Module):\n",
        "    def __init__(self, dilation=1):\n",
        "        super(causalFCN, self).__init__()\n",
        "        size = 64\n",
        "        n_channels = 1\n",
        "        n_classes = 1\n",
        "        self.inc = Double_conv_causal(n_channels, size)\n",
        "        self.down1 = Down_causal(size, 2*size)\n",
        "        self.down2 = Down_causal(2*size, 4*size)\n",
        "        self.down3 = Down_causal(4*size, 8*size, pooling_kernel_size=5, pooling_stride=5)\n",
        "        self.down4 = Down_causal(8*size, 4*size, pooling=False, dilation=2)\n",
        "        self.up2 = Up_causal(4*size, 2*size, kernel_size=5, stride=5)\n",
        "        self.up3 = Up_causal(2*size, size)\n",
        "        self.up4 = Up_causal(size, size)\n",
        "        self.outc = Outconv(size, n_classes)\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up2(x5, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        x = self.outc(x)\n",
        "        return x\n",
        "\n",
        "# Exemple d'utilisation\n",
        "model = causalFCN()\n",
        "# Série temporelle d'entrée (x_t):\n",
        "input_tensor1 = torch.rand(1, 1, 10000)\n",
        "# Série temporelle en sortie f(x_t):\n",
        "output = model(input_tensor1)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "id": "fIbU1EJT1MM9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5868f421-143a-474e-e76c-5609d81057cd"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 10000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** De quel type de réseau de neurones s'agit-il ? Combien de paramètres la couche self.Down1 compte-t-elle (à faire à la main) ?\n",
        "Combien de paramètres le réseau entier compte-t-il (avec un peu de code) ?"
      ],
      "metadata": {
        "id": "-mNnsYU-7R7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nb de paramètres dans self.Down1: (calcul \"à la main\")\n",
        "# self.down1 = Down_causal(size, 2*size)\n",
        "# Input channels: size (64), Output channels: 2*size (128)\n",
        "# Each convolution layer:\n",
        "# -> Weight parameters: out_ch * in_ch * kernel_size\n",
        "# -> Bias parameters: out_ch\n",
        "conv1_params = (128 * 64 * 3) + 128  # First Conv1d layer in Double_conv_causal\n",
        "conv2_params = (128 * 128 * 3) + 128  # Second Conv1d layer in Double_conv_causal\n",
        "bn1_params = (128 * 2)  # BatchNorm1d (128 weights + 128 biases)\n",
        "bn2_params = (128 * 2)  # BatchNorm1d (128 weights + 128 biases)\n",
        "down1_total_params = conv1_params + conv2_params + bn1_params + bn2_params\n",
        "print(f\"Nb de paramètres dans self.Down1: {down1_total_params}\")\n",
        "\n",
        "# Nb de paramètres au total:\n",
        "model = causalFCN()\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Nb de paramètres au total: {total_params}\")"
      ],
      "metadata": {
        "id": "qlYxUf6U9vH1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d78cbbb0-b42c-44e8-aede-9db7d9da7cd3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nb de paramètres dans self.Down1: 74496\n",
            "Nb de paramètres au total: 2872641\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** Par quels mécanismes la taille du vecteur d'entrée est-elle réduite ? Comment est-elle restituée dans la deuxième partie du réseau ?"
      ],
      "metadata": {
        "id": "I4D46A0-8LaV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANSWER:\n",
        "The input vector size is reduced through max-pooling in layers like self.down1, which downsample the temporal resolution, and through increased channel depth in convolutional layers. It is restored in the second part of the network using transposed convolutions in layers like self.up2, which upsample the temporal resolution, and through skip connections that combine high-resolution features from earlier layers with the upsampled output."
      ],
      "metadata": {
        "id": "SwBsjHVmdREm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Par quels mécanismes le champ réceptif est-il augmenté ? Préciser par un calcul la taille du champ réceptif en sortie de *self.inc*."
      ],
      "metadata": {
        "id": "SVNeFnm88yV2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANSWER :\n",
        "The receptive field is increased through **dilation**, **kernel size** in convolutional layers, and **max-pooling** in the downsampling path. For `self.inc`, which uses two convolutional layers with a kernel size of 3 and dilation of 1, the receptive field grows as follows: the first convolution has a receptive field of 3, and the second convolution expands it to \\( 3 + (3 - 1) = 5 \\). Therefore, the receptive field of `self.inc` is 5.\n"
      ],
      "metadata": {
        "id": "XSr8BSftdpd0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4** Par un bout de code, déterminer empiriquement la taille du champ réceptif associé à la composante $y_{5000}$ du vecteur de sortie. (Indice: considérer les sorties associées à deux inputs qui ne diffèrent que par une composante...)"
      ],
      "metadata": {
        "id": "TVVcBPuA9EP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create two inputs differing at a single index\n",
        "input1 = torch.zeros(1, 1, 10000)\n",
        "input2 = input1.clone()\n",
        "input2[0, 0, 4990] = 1  # Modify a point near index 5000\n",
        "\n",
        "# Pass inputs through the network\n",
        "output1 = model(input1)\n",
        "output2 = model(input2)\n",
        "\n",
        "# Find the indices where the output at 5000 differs\n",
        "diff = torch.abs(output1 - output2)\n",
        "receptive_field = (diff[0, 0, :] > 1e-5).nonzero().max() - (diff[0, 0, :] > 1e-5).nonzero().min() + 1\n",
        "\n",
        "print(f\"Receptive field size: {receptive_field.item()}\")\n"
      ],
      "metadata": {
        "id": "69WMWCSZAg5_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a567533d-f7b8-49d1-ac6b-28278ce72718"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Receptive field size: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5** $y_{5000}$ dépend-elle des composantes $x_{t, \\space t > 5000}$ ? Justifier de manière empirique puis préciser la partie du code de Double_conv_causal qui garantit cette propriété de \"causalité\" en justifiant.  \n",
        "\n"
      ],
      "metadata": {
        "id": "gZ37skwm-Vpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create two inputs\n",
        "input1 = torch.zeros(1, 1, 10000)\n",
        "input2 = input1.clone()\n",
        "input2[0, 0, 5010] = 1  # Modify a point after index 5000\n",
        "\n",
        "# Pass inputs through the network\n",
        "output1 = model(input1)\n",
        "output2 = model(input2)\n",
        "\n",
        "# Check if y[5000] changes\n",
        "y5000_diff = torch.abs(output1[0, 0, 5000] - output2[0, 0, 5000]).item()\n",
        "print(f\"y[5000] difference: {y5000_diff}\")\n"
      ],
      "metadata": {
        "id": "PeooRYE-ATGt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "510ddc6c-bc3e-44ed-c23f-cf34202b1eef"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y[5000] difference: 0.004481717944145203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANSWER :\n",
        "$y_{5000}$ does not depend on $x_t, t > 5000$, and $y_{5000\\_diff}$ is almost zero, confirming causality.\n",
        "The property of \"causality\" is guaranteed by the manual padding in the forward method  in Double_conv_causal: x = F.pad(x, ((self.kernel_size - 1) * self.dilation, 0))\n",
        "This padding adds extra elements only to the left of the input sequence, ensuring that each convolutional layer only uses past and present values.\n"
      ],
      "metadata": {
        "id": "PAga6-PrfDN8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "---\n",
        "\n",
        "\\"
      ],
      "metadata": {
        "id": "qV52tusgNn6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "Exercice 3: \"Ranknet loss\""
      ],
      "metadata": {
        "id": "bm-sRzmfqc2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un [article récent](https://https://arxiv.org/abs/2403.14144) revient sur les progrès en matière de learning to rank. En voilà un extrait :"
      ],
      "metadata": {
        "id": "Wl8wUjsSM57D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src=\"https://raw.githubusercontent.com/nanopiero/exam_2025/refs/heads/main/utils/png_exercice3.PNG\" alt=\"extrait d'un article\" width=\"800\">"
      ],
      "metadata": {
        "id": "SDZUXMlSDpoe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** Qu'est-ce que les auteurs appellent \"positive samples\" et \"negative samples\" ? Donner un exemple."
      ],
      "metadata": {
        "id": "9NzV1PbMNyuo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANSWER :\n",
        "\n",
        "In the context of the text, positive samples refer to documents or items that are relevant to a given query, such as a highly relevant search result for the user's input. On the other hand, negative samples are documents or items that are irrelevant or less relevant to the query, such as a poorly ranked or unrelated search result."
      ],
      "metadata": {
        "id": "gNiyvdbHgY1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** Dans l'expression de $\\mathcal{L}_{RankNet}$, d'où proviennent les $z_i$ ? Que représentent-ils ?  "
      ],
      "metadata": {
        "id": "yIKQ5Eo9OnPq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANSWER:\n",
        "\n",
        "In the expression for $\\mathcal{L}_{RankNet}$, the $z_i$ values are the **predicted scores** or **relevance scores** assigned by the model to the input documents or items. These scores are computed by the ranking model to quantify how relevant each document $i$ is to the given query. Specifically, $z_i$ represents the output of the model for document $i$, typically obtained through a scoring function such as a neural network. These scores are used to compare pairs of documents ($z_i - z_j$) to determine the relative ordering of relevance between them, which forms the basis of the pairwise loss in RankNet.\n",
        "\n"
      ],
      "metadata": {
        "id": "7gYy9-ENgx3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Pourquoi cette expression conduit-elle à ce que, après apprentissage, \"the estimated\n",
        "value of positive samples is greater than that of negative samples\n",
        "for each pair of positive/negative samples\" ?"
      ],
      "metadata": {
        "id": "r74fWiyvPb7Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANSWER:\n",
        "\n",
        "The expression for $\\mathcal{L}_{RankNet}$ minimizes the pairwise loss by adjusting the model's predicted scores $z_i$ and $z_j$ such that $\\sigma(z_i - z_j)$ approaches $1$ when $y_{ij} = 1$, indicating that the model predicts $z_i > z_j$ for a positive-negative pair. Conversely, $\\sigma(z_i - z_j)$ approaches $0$ when $y_{ij} = 0$, indicating $z_i < z_j$. This optimization ensures that, after learning, the predicted score $z_i$ for a p\n"
      ],
      "metadata": {
        "id": "OF2-rsMehBlS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4** Dans le cadre d'une approche par deep learning, quels termes utilise-t-on pour qualifier les réseaux de neurones exploités et la modalité suivant laquelle ils sont entraînés ?"
      ],
      "metadata": {
        "id": "pk1EIi_VVi3R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANSWER:\n",
        "\n",
        "Dans le cadre d'une approche par deep learning, les réseaux de neurones exploités pour ce type de tâche sont qualifiés de **réseaux de classement** (*ranking networks*), tels que RankNet, et sont conçus pour effectuer des comparaisons de pertinence entre des paires ou des ensembles de documents. La modalité suivant laquelle ces réseaux sont entraînés est appelée **apprentissage par paires** (*pairwise learning*), où l'objectif est de minimiser une perte basée sur les différences de scores entre des paires de documents, en s'assurant que les exemples positifs obtiennent des scores supérieurs à ceux des exemples négatifs.\n"
      ],
      "metadata": {
        "id": "1TlONCU3hOTm"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}